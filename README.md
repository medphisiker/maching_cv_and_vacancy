![home_page](src/streamlit_app/pics/CareerRank.jpg)

# CareerRank, - сервис для подбора походящих друг другу вакансий и резюме

# Описание проекта

CareerRank - помогаем талантам найти места для их реализации!

Мы предлагаем сервис для подбора походящих друг другу вакансий и резюме.
Мы используем нейросетевые языковые модели (LM) для подбора 
подходящих друг другу вакансий и резюме на основе их текстовых описаний.

Сервис использует модель работающую с русском и английским языком.
Это актуально для ит-вакансий и резюме, когда текст описания может быть на русском языке, но содержать термины из ит области на английском языке, например `DevOps`, `Data Scientist` и т.д.

![sevice_scheme](reports/figures/CareerRank.gif)

Видео демонстрация работы сервиса([ссылка](https://youtu.be/ThIdllGH9ug)).

# Схема работы сервиса

![sevice_scheme](reports/figures/sevice_scheme.png)

## Построение поискового индекса Faiss

Мы берем текстовые описания (`src/models`) для
* вакансий из колонок "name", "description"
* резюме из колонок "Ищет работу на должность:", "Опыт работы"

получаем эмбединги текстовых описаний с помощью предобученных моделей:
* `cointegrated/rubert-tiny2` ([ссылка](https://huggingface.co/cointegrated/rubert-tiny2))

* `DeepPavlov/distilrubert-tiny-cased-conversational` ([ссылка](https://huggingface.co/DeepPavlov/distilrubert-tiny-cased-conversational))

* `cointegrated/LaBSE-en-ru` ([ссылка](https://huggingface.co/cointegrated/LaBSE-en-ru))

Строим индекс Facebook Faiss отдельно для всех вакансий и отдельно для всех резюме.

Сохраняем их в папку `src/streamlit_app/data`.

## Запуск Streamlit web-приложения

Теперь мы можем запускать web-приложение Streamlit (`src/streamlit_app`) командой

```
cd src/streamlit_app
streamlit run homepage.py
```

Перейдя по адресу показанному в терминале Streamlit в нашем браузере откроется home страница веб-приложения:

![home_page](reports/figures/home_page.jpg)

# Структура проекта

Структура проекта была создана под вдохновением от `Cookiecutter Data Science template` ([link](https://github.com/drivendata/cookiecutter-data-science)).

Исходная структура данного шаблона показалась нам перегруженной для небольшого проекта со сроком выполнения 2 недели.

Мы создали упрощенный вариант, который всегда может быть расширен, компонентами структуры исходного шаблона по мере необходимости и развития проекта.

Структура нашего проекта выгладит так

```
├── LICENSE            <- The MIT License (MIT).
├── README.md          <- Описание проекта README ля других разработчиков использующих проект.
├── data
│   ├── analysis       <- Данные используемые для анализа. Например, кропы с представителями классов дорожных знаков.
│   ├── external       <- Данные из других источников. Например, видео с YouTube для теста работы нашей модели.
│   ├── interim        <- Преобразованные датасеты для более удобного использования, но еще не в форме для обучения модели.
│   ├── processed      <- Финальная версия датасета используемая для обучения моделей.
│   └── raw            <- Датасеты в своей исходной форме.
│
├── models             <- Обученные и сериализованные модели и их метрики.
│
├── notebooks          <- Jupyter notebooks. Ноутбуки для исследования и анализа данных.
│
├── reports            <- Отчеты по анализу данных в виде Excel, HTML, PDF, LaTeX, etc.
│   └── figures        <- Графики и диаграммы в виде картинок используемые для отчетов или документации.
│
├── pyproject.toml     <- Мы используем poetry для создания виртуальной среды для разработки.
│
├── src                <- Source code используемый в данном проекте.
    │
    ├── data           <- Скрипты для обработки данных
    │
    ├── models         <- Скрипты для обучения моделей и упаковки их в архив для сохранения в облаке.
    │
    └── visualization  <- Скрипты для визуализации данных. Например, для визуализации кропов классов дорожных знаков.
```

# Развернуть рабочее окружение

Проект использует `poetry` для управления зависимостями.
Устанавливаем `poetry` в нашу систему если он не установлен согласно офф. документации ([ссылка](https://python-poetry.org/docs/)).

Для создания виртуального рабочего окружения с `poetry` выполняем команду:
```
poetry install
```

Для его активации выполняем команду:
```
poetry shell
```

# Использованные датасеты

Мы использовали датасеты:
 * с базой данных резюме с HeadHunter.ru ([ссылка](https://drive.google.com/file/d/1ikA_Ht45fXD2w5dWZ9sGTSRl-UNeCVub/view?usp=share_link))
 * с базой данных вакансий с HeadHunter.ru ([ссылка](https://t.me/c/1994322130/450))
